# nowcast/pipeline/run_gdp_nowcast.py

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm  # è¿›åº¦æ¡åº“ï¼Œå¦‚æœæ²¡æœ‰è¯· pip install tqdm

from nowcast.data.fred import FredDataProvider
from nowcast.features.targets import get_target_series
from nowcast.features.panel_builder import PanelBuilder
from nowcast.features.asof_dataset import AsOfDatasetGenerator

from nowcast.models.ridge import GDPNowcasterRidge
from nowcast.models.svr import GDPNowcasterSVR

from sklearn.metrics import r2_score, mean_squared_error

def run_backtest(start_date="2015-01-01", end_date=None, freq="M"):
    """
    è¿è¡Œå…¨é‡å†å²å›æµ‹ (Backtest/Vintage Replay)ã€‚
    
    é›†æˆç‰¹æ€§ï¼š
    1. é¢„è®¡ç®— (Pre-computation): å°†å¤æ‚åº¦ä» O(N^2) é™ä¸º O(N)ï¼Œæå¤§åŠ é€Ÿå›æµ‹ã€‚
    2. è‡ªåŠ¨è°ƒä¼˜ (Auto-tune): è°ƒç”¨ RandomizedSearchCV å¯»æ‰¾æœ€ä½³ C å’Œ epsilonã€‚
    3. å®Œæ•´è¯„ä¼°: åŒ…å« R2, MSE, Variance Ratio å’Œæ®‹å·®åˆ†æã€‚
    """
    print("ğŸš€ Initializing Nowcast Pipeline...")
    
    # 1. å‡†å¤‡å…¨é‡æ•°æ®
    # ä½¿ç”¨ 'offline_mode' åˆ©ç”¨æœ¬åœ°ç¼“å­˜ (éœ€ç¡®ä¿ update_data.py å·²è¿è¡Œ)
    provider = FredDataProvider(api_key="offline_mode") 
    
    # 2. æ„å»ºç›®æ ‡ (y)
    y_full = get_target_series(provider)
    # [å…³é”®ä¿®å¤] åˆ é™¤å› è®¡ç®—å¢é•¿ç‡äº§ç”Ÿçš„é¦–è¡Œ NaNï¼Œå¦åˆ™ SVR ä¼šæŠ¥é”™
    y_full = y_full.dropna()

    # 3. æ„å»ºç‰¹å¾é¢æ¿ (X)
    # ä» yaml è‡ªåŠ¨è¯»å–æ‰€æœ‰ features
    features_list = [k for k in provider.series_config.keys() if k != 'gdp_real']
    panel_full = PanelBuilder(provider).build_monthly_panel(features_list)
    
    # 4. åˆå§‹åŒ–æ•°æ®é›†ç”Ÿæˆå™¨
    gen = AsOfDatasetGenerator(panel_full, y_full)
    
    # 5. ç”Ÿæˆè¯„ä¼°æ—¶é—´è½´
    if end_date is None:
        end_date = pd.Timestamp.now()
    
    eval_dates = pd.date_range(start=start_date, end=end_date, freq=freq)
    
    print(f"ğŸ“… Starting Vintage Replay from {start_date} to {str(end_date)[:10]}...")
    
    # ==========================================
    # [åŠ é€Ÿä¼˜åŒ–] 1. é¢„è®¡ç®—å†å²è®­ç»ƒæ ·æœ¬ (Pre-computation)
    # ==========================================
    print("âš¡ Pre-computing historical feature vectors to speed up training...")
    
    historical_X_map = {}
    
    # éå†æ‰€æœ‰å·²çŸ¥çš„çœŸå® GDP å­£åº¦ï¼Œé¢„å…ˆç”Ÿæˆå¯¹åº”çš„ç‰¹å¾å‘é‡
    # å› ä¸ºè®­ç»ƒé›†ä½¿ç”¨çš„æ˜¯ Revised Dataï¼Œè¿™éƒ¨åˆ†æ˜¯å›ºå®šçš„ï¼Œä¸éœ€è¦åœ¨å¾ªç¯é‡Œé‡å¤è®¡ç®—
    for q_date in y_full.index:
        q_months = gen.get_quarter_months(q_date)
        X_vec = gen.create_quarterly_feature_vector(q_months, panel_full)
        historical_X_map[q_date] = X_vec
        
    results = []
    print(f"   Total evaluation points: {len(eval_dates)}")

    # ==========================================
    # ä¸»å¾ªç¯ (Time Travel Loop)
    # ==========================================
    for as_of_date in tqdm(eval_dates):
        # --- A. å‡†å¤‡è®­ç»ƒé›† (Training Set) ---
        # è§„åˆ™ï¼šåªèƒ½ç”¨ as_of_date ä¹‹å‰å·²ç»"å®Œç»“"ä¸”GDPå·²å…¬å¸ƒçš„å­£åº¦è¿›è¡Œè®­ç»ƒ
        # å‡è®¾ GDP å‘å¸ƒå»¶è¿Ÿ 90 å¤©
        training_cutoff = as_of_date - pd.Timedelta(days=90)
        
        # ç­›é€‰åˆæ³•çš„è®­ç»ƒå­£åº¦
        valid_quarters = y_full.index[y_full.index <= training_cutoff]
        
        if len(valid_quarters) < 12: 
            # å¦‚æœè®­ç»ƒæ ·æœ¬å¤ªå°‘ (æ¯”å¦‚åˆšå¼€å§‹å›æµ‹æ—¶)ï¼Œè·³è¿‡
            continue
            
        # [æé€Ÿæ¨¡å¼] ç›´æ¥æŸ¥è¡¨è·å– X_trainï¼Œä¸å†é‡å¤è®¡ç®—
        # ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼ä»é¢„è®¡ç®—çš„å­—å…¸ä¸­æå–ï¼Œé€Ÿåº¦æå¿«
        X_train_list = [historical_X_map[q] for q in valid_quarters]
        # y_train ç›´æ¥åˆ‡ç‰‡
        y_train_list = y_full.loc[valid_quarters].values
        
        X_train = np.array(X_train_list)
        y_train = np.array(y_train_list)
        
        # --- B. å‡†å¤‡é¢„æµ‹æ ·æœ¬ (Test Sample) ---
        # Test Sample å¿…é¡»ä¿æŒ Vintage Logic (Ragged Edge)ï¼Œä¸èƒ½é¢„è®¡ç®—
        # å› ä¸ºæ¯ä¸ª as_of_date çœ‹åˆ°çš„æ•°æ®ç¼ºå¤±æƒ…å†µéƒ½ä¸ä¸€æ ·
        current_sample_list = gen.generate_dataset([as_of_date])
        test_sample = current_sample_list[0]
        
        X_test = test_sample.X.reshape(1, -1) # SVR è¦æ±‚ 2D array
        
        # --- C. è®­ç»ƒä¸é¢„æµ‹ ---

        # é€‰æ‹©åˆ‡æ¢æ¨¡å‹
        model = GDPNowcasterRidge()
        model.fit(X_train, y_train)
        
        pred_value = model.predict(X_test)[0]
        
        # --- D. è®°å½•ç»“æœ ---
        results.append({
            "date": as_of_date,
            "target_quarter": test_sample.quarter_label,
            "nowcast": pred_value,
            "train_size": len(y_train)
        })

    # ==========================================
    # ç»“æœæ•´åˆä¸å¯è§†åŒ–
    # ==========================================
    df_res = pd.DataFrame(results).set_index("date")
    
    print("\nâœ… Backtest Complete!")
    
    # --- 1. æ•°æ®å¯¹é½ä¸è¯„ä¼° ---
    # æ„å»ºçœŸå®å€¼æŸ¥æ‰¾è¡¨
    y_truth_map = y_full.to_dict()
    
    def get_truth(q_str):
        q_ts = pd.Timestamp(q_str)
        return y_truth_map.get(q_ts, np.nan)

    # æ˜ å°„çœŸå®å€¼
    df_res['actual'] = df_res['target_quarter'].apply(get_truth)
    
    # [å…³é”®ä¿®å¤] ä½¿ç”¨ .copy() é¿å… SettingWithCopyWarning
    df_eval = df_res.dropna(subset=['actual']).copy()
    
    if len(df_eval) > 0:
        # --- 2. è®¡ç®—æ ¸å¿ƒæŒ‡æ ‡ ---
        r2 = r2_score(df_eval['actual'], df_eval['nowcast'])
        mse = mean_squared_error(df_eval['actual'], df_eval['nowcast'])
        var_pred = np.var(df_eval['nowcast'])
        var_actual = np.var(df_eval['actual'])
        
        print("\nğŸ“Š Model Performance Metrics:")
        print(f"   RÂ² Score         : {r2:.4f} (è¶Šæ¥è¿‘1è¶Šå¥½ï¼Œè´Ÿæ•°è¯´æ˜ä¸å¦‚ççŒœ)")
        print(f"   MSE              : {mse:.6f}")
        print(f"   Variance (Pred)  : {var_pred:.6f}")
        print(f"   Variance (Actual): {var_actual:.6f}")
        # å¦‚æœ y è¢«æ”¾å¤§äº†100å€ï¼Œè¿™é‡Œä¸éœ€è¦å†è°ƒæ•´å•ä½
        print(f"   Var Ratio (P/A)  : {var_pred/var_actual:.2f} (è¿‡ä½è¯´æ˜æ¬ æ‹Ÿåˆ/æ­»é±¼çº¿)")
        
        # --- 3. æ®‹å·®åˆ†æ ---
        df_eval['residual'] = df_eval['nowcast'] - df_eval['actual']
        
        print("\nğŸ“‰ Top 5 Largest Errors (Residuals):")
        print(df_eval['residual'].abs().sort_values(ascending=False).head(5))
        
        # ç»˜å›¾
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True, gridspec_kw={'height_ratios': [2, 1]})
        
        # ä¸Šå›¾ï¼šé¢„æµ‹ vs çœŸå®
        ax1.plot(df_res.index, df_res['nowcast'], label='GDP Nowcast', color='blue', linewidth=1.5)
        
        # ç»˜åˆ¶çœŸå®å€¼ (çº¢ç‚¹)
        y_truth_plot = y_full[y_full.index >= pd.to_datetime(start_date)]
        ax1.plot(y_truth_plot.index, y_truth_plot, 'ro', label='Actual GDP', markersize=4)
        ax1.step(y_truth_plot.index, y_truth_plot, where='post', color='red', alpha=0.3, linestyle='--')
        
        ax1.set_title(f"US Real GDP Nowcast (SVR Auto-Tuned) | RÂ²={r2:.3f}")
        ax1.set_ylabel("QoQ Annualized Growth (%)")
        ax1.grid(True, alpha=0.3)
        ax1.legend()
        
        # ä¸‹å›¾ï¼šæ®‹å·®
        ax2.bar(df_eval.index, df_eval['residual'], color='gray', alpha=0.6, width=20, label='Residual (Pred - Actual)')
        ax2.axhline(0, color='black', linewidth=0.8)
        ax2.set_ylabel("Error (%)")
        ax2.set_title("Residual Analysis")
        ax2.grid(True, alpha=0.3)
        ax2.legend()
        
        plt.tight_layout()
        output_path = "nowcast_result_with_metrics.png"
        plt.savefig(output_path)
        print(f"\nğŸ“Š Chart saved to {output_path}")
        
    else:
        print("âš ï¸ Not enough data points to calculate metrics.")

    return df_res

if __name__ == "__main__":
    # è¿è¡Œå›æµ‹
    run_backtest(start_date="2015-01-01", freq="M")